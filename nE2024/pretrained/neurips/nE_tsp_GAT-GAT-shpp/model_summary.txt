VRPModel(
  (_encoder): AttentionEncoder(
    (init_embed): InitEncoder(
      (init_embed): Linear(in_features=2, out_features=128, bias=True)
    )
    (embedder): GraphAttentionEncoder(
      (layers): Sequential(
        (0): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (_decoder): AttentionDecoder(
    (project_node_embeddings): Linear(in_features=128, out_features=384, bias=False)
    (project_fixed_context): Linear(in_features=128, out_features=128, bias=False)
    (project_step_context): Linear(in_features=256, out_features=128, bias=False)
    (project_out): Linear(in_features=128, out_features=128, bias=False)
    (project_glimpse): Linear(in_features=128, out_features=128, bias=False)
  )
  (encoder): AttentionEncoder(
    (init_embed): InitEncoder(
      (init_embed): Linear(in_features=2, out_features=128, bias=True)
    )
    (embedder): GraphAttentionEncoder(
      (layers): Sequential(
        (0): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (2): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (3): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (4): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (5): MultiHeadAttentionLayer(
          (0): SkipConnection(
            (module): MultiHeadAttention(
              (edge_mlp): MLP(
                (0): Linear(in_features=1, out_features=16, bias=True)
                (1): ReLU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=16, out_features=16, bias=True)
                (4): ReLU()
                (5): Dropout(p=0.0, inplace=False)
                (6): Linear(in_features=16, out_features=8, bias=True)
                (7): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (2): SkipConnection(
            (module): Sequential(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): ReLU()
              (2): Linear(in_features=512, out_features=128, bias=True)
            )
          )
          (3): Normalization(
            (normalizer): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
      )
    )
  )
  (decoder): AttentionDecoder(
    (project_node_embeddings): Linear(in_features=128, out_features=384, bias=False)
    (project_fixed_context): Linear(in_features=128, out_features=128, bias=False)
    (project_step_context): Linear(in_features=256, out_features=128, bias=False)
    (project_out): Linear(in_features=128, out_features=128, bias=False)
    (project_glimpse): Linear(in_features=128, out_features=128, bias=False)
  )
)

PARAMS: [256, 128, 16384, 16384, 16384, 16384, 16, 16, 256, 16, 128, 8, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 16384, 16384, 16384, 16, 16, 256, 16, 128, 8, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 16384, 16384, 16384, 16, 16, 256, 16, 128, 8, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 16384, 16384, 16384, 16, 16, 256, 16, 128, 8, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 16384, 16384, 16384, 16, 16, 256, 16, 128, 8, 128, 128, 65536, 512, 65536, 128, 128, 128, 16384, 16384, 16384, 16384, 16, 16, 256, 16, 128, 8, 128, 128, 65536, 512, 65536, 128, 128, 128, 256, 49152, 16384, 32768, 16384, 16384]

TOTAL PARAMS: 1320912